# Welcome to My Convex Optimization
***My Convex Optimization***



## Task
To find the minimum of our little f function, we are going to use its derivative f' (f prime). The zero of f prime (the point where it evaluates to zero) matches with the minimum of the f function.

Our goal is hence to find where f prime cancels out.

Write a simple dichotomous algorithm (bisection method) to find the zero of a function.

Write a simple gradient descent function which finds the minimum of a function
Gradient Descent methods are the workhorse of machine learning, from linear regression to deep neural nets. Here, we used it in a one dimension problem, but it can be used with any number of dimensions !

Adding linear constraints to a convex function does not change its convexity, it remains convex. What it does though is restricting the space of solutions by intersecting it with hyperplanes. If our convex function solution space is a spherical orange, applying linear constraints is like slicing the orange. It remains convex but sharper.

We are going to solve this linear problem with the Simplex method. The simplex algorithm is pretty straightforward: it moves from a vertex to another until it finds a solution which maximizes the objective function. 

### The Core Team


<span><i>Made at <a href='https://qwasar.io'>Qwasar SV -- Software Engineering School</a></i></span>
<span><img alt='Qwasar SV -- Software Engineering School's Logo' src='https://storage.googleapis.com/qwasar-public/qwasar-logo_50x50.png' width='20px'></span>
